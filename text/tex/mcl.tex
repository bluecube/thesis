\chapter{Monte Carlo Localization}
\label{chap:mcl}

The following chapter defines the localization problem, provides overview
of Markov Localization and Kalman Filters and finally describes
Monte Carlo Localization algorithm.

\section{Localization}
Localization of a mobile robot is a task of estimating position (and possibly
also other state) of the robot based on performed actions and sensor readings.
To reliably navigate its environment autonomous mobile robot needs
a relatively precise estimate of its position.

Localization methods can be categorized according to many different criteria.
One of the possibilities is categorization based on whether the environment is static
or dynamic.

Another dimension along which the localization algorithms can be grouped
is the ability of the localization algorithm to modify behavior of the robot
in order to find the current position faster or with more precision.

Next, localization can mean either position tracking, where the robot knows its initial
position and the task is only to update the estimate and handle relatively small
errors, or global localization, where the robot has to find its position from scratch
and recover from serious localization errors.

In this work we will focus only on passive localization in static environment and
since Monte Carlo Localization is a topic of this thesis we will be favoring
global localization, although typically GPS is used with Kalman filters
which are only capable of tracking the estimate locally.

\section{Markov Localization}

Markov Localization \cite{fox98,diard03} is a recursive probabilistic
algorithm that estimates state of the robot based on its
actions and possibly noisy measured sensor data.

\subsection{Derivation}

Markov localization assumes that the state of the robot only depends
on state and action performed in the previous time frame
%	\begin{equation}
%		\Prob(X_i = x \mid X_{1, \dotsc, i - 1}, a_{1, \dotsc, i - 1},
%		o_{1, \dotsc, i - 1}) = 
%		\Prob(X_i = x \mid X_{i - 1}, a_{i - 1})
%	\end{equation}
and that each observation only depends on the current state.
%	\begin{equation}
%		\Prob(o_i \mid X_1, \dotsc, X_i, a_1, \dotsc, a_{i - 1},
%		o_1, \dotsc, o_{i - 1}) = 
%		\Prob(o_i \mid X_i)
%	\end{equation}

It computes probability density of the robot state, called belief.
\begin{equation}
	\label{eq:markov-belief-def}
	\Bel(X_i = x) = \Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1,\dotsc,i - 1})
\end{equation}
Here \(X_i\) is the random variable representing robot's state,
\(o_i\) is the observed sensor input in time step \(i\) and \(a_i\)
is the action the robot performs in the time step \(i\), after it
measures \(o_i\).

Using Bayes rule, equation \eqref{eq:markov-belief-def} can be transformed to
\begin{equation}
	\label{eq:markov-derivation1}
	\Bel(X_i = x) =
	\frac{
		\Prob(o_i \mid X_i = x, o_{1, \dotsc, i}, a_{1,\dotsc,i - 1})
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
	}{
		\Prob(o_i \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})
	}
\end{equation}

Note that denominator \(\Prob(o_i \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})\) only
serves as a normalization constant.
In further equations it will be replaced by \(\eta^{-1}\).
Other than that, we can use the independence assumptions made at the beginning
of this section and simplify \eqref{eq:markov-derivation1} to
\begin{equation}
	\label{eq:markov-derivation2}
	\Bel(X_i = x) =
		\eta \Prob(o_i \mid X_i = x)
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
\end{equation}
%The equation \eqref{eq:markov-derivation1} has already been simplified using
%the independence assumptions.
By integrating over all possible states in time \(i - 1\), the rightmost term in
equation \eqref{eq:markov-derivation1} can be expanded in a following way:
\begin{multline}
	\label{eq:markov-derivation3}
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) = \\
	\int
	\Prob(X_i = x \mid X_{i - 1}=x', o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
	\Prob(X_{i-1} = x' \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})
	\,\mathrm{d}x'
\end{multline}
After substituting definition of belief from equation \eqref{eq:markov-belief-def}
and another use the independence assumptions we get
\begin{equation}
	\label{eq:markov-derivation4}
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) =
	\int
	\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})
	\Bel(X_{i-1} = x')
	\,\mathrm{d}x'
\end{equation}
Finally, substituting into \eqref{eq:markov-derivation2} gives us the recursive
equation
\begin{equation}
	\label{eq:markov-final}
	\Bel(X_i = x) =
	\eta \Prob(o_i \mid X_i = x)
		\int
		\Prob(X_i = x' \mid X_{i - 1}=x', a_{i - 1})
		\Bel(X_{i-1} = x')
		\,\mathrm{d}x'
\end{equation}

We will call the conditional density \(\Prob(o_i \mid X_i = x)\) \emph{sensor model} and
the density \(\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})\) \emph{motion model}.
Sensor model describes the probability of observing \(o_i\) at given time.
It implicitly contains map of the environment and models interactions of sensors with
the map.
Action model contains information about how the robot's actions relate to changes in its state.

\subsection{Algorithm}
The algorithm for Markov localization operates in two alternating phases:
prediction phase, in witch the algorithm incorporate a performed action into the belief
(and therefore predicts the state after the action and correction phase which updates
the measurements based on the observed sensor measurements.

\begin{description}
\item[Prediction] \hfill \\
	Prediction phase uses the action model to obtain predictive density based on the action performed
	and previous belief, by integrating over all possible states at time \(i - 1\).
	The following equation is an exact copy of \eqref{eq:markov-derivation4}, included here for completeness.
	\begin{equation}
		\label{eq:markov-prediction}
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) =
		\int
		\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})
		\Bel(X_{i-1} = x')
		\,\mathrm{d}x'
	\end{equation}

\item[Correction] \hfill \\
	Correction regenerates the belief based on the predictive density
	and sensor model.
	\begin{equation}
		\label{eq:markov-correction}
		\Bel(X_i = x) =
		\eta \Prob(o_i \mid X_i = x)
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
	\end{equation}
\end{description}

\subsection{Density Representations}
The algorithm as described above doesn't tell us how to represent the probability
densities encountered.
Several specialisations of Markov localization exists differing mainly in how the
current state estimation is represented.
Examples include Gaussian distribution in Kalman filters,
%(together with other assumptions, see \ref{sec:kalman}),
grid based algorithms \cite{fox98}, or representing the distribution
using samples in Monte Carlo localization (see section \ref{sec:mcl-algorithm}).

\subsection{Kalman Filter}
\label{sec:kalman}
Kalman filter \cite{kalman60,welch95} is a popular state estimator, often used in robotics
for localization.
It can be viewed as a closed form solution of Markov localization where both the
action model and measurement model are linear and Gaussian \cite{diard03}.

\begin{gather}
	\label{eq:kalman-action-model}
	\Prob(X_i = x \mid X_{i-1} = x', a_{i - 1}) \sim \mathrm{N}(A_{i - 1}x' + B_{i - 1}a_{i - 1}, Q_{i - 1})
	\\
	\label{eq:kalman-measurement-model}
	\Prob(o_{i} \mid X_i = x) \sim \mathrm{N}(H_{i}x, R_i)
\end{gather}
In equations \eqref{eq:kalman-action-model} and \eqref{eq:kalman-measurement-model}
the matrix \(A_i\) describes change of state if there was no control input and no noise,
matrix \(B_i\) contains the influence of action \(a_i\) and \(H_i\) relates state to measured
values.
\(Q_i\) and \(R_i\) are covariance matrices of action model and measurement model.

Because the initial belief, the action model and the measurement model are all Gaussian,
belief will always remain Gaussian as well.
This means, that only mean value and covariance matrix need to be stored, making
Kalman filter very efficient, requiring only several matrix operations in each step.

Non linear motion and measurement models can be approximated
using a first order Taylor expansion to form extended Kalman
filter (EKF) \cite{welch95}.

For illustration of Kalman filter operation, the update rules are included,
for more detailed description see \cite{welch95}.

\begin{description}
\item[Prediction] \hfill \\
\begin{gather}
	\bar{x}^{-}_i = A_{i - 1}\bar{x}_{i-1} + B_{i - 1}a_{i - 1}
	\\
	P^{-}_i = A_{i - 1}P_{i - 1}A^\mathrm{T}_{i - 1} + Q_{i - 1}
\end{gather}

Here \(\bar{x}_i\) and \(P_i\) are mean and covariance describing \(\Bel(X_i)\).
\(\bar{x}^{-}_i\) and \(P^{-}_i\) are mean and covariance of the predictive density
\(\Prob(X_i \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})\)

\item[Correction] \hfill \\
\begin{gather}
	\label{eq:kalman-gain}
	K_i = P^{-}_{i}H^\mathrm{T}_{i}(H_{i}P^{-}_{i}H^\mathrm{T}_{i} + R_{i})^{-1}
	\\
	\bar{x}_i = \bar{x}^{-}_i + K (o_i - H_{i}\bar{x}^{-}_i)
	\\
	P_i = (I - K_{i}H_{i})P^{-}_i
\end{gather}
\end{description}


\section{Monte Carlo Localization}
\label{sec:mcl-algorithm}

\marginpar{Obrazek! aspon ten z bakalarky}

Monte Carlo Localization \cite{dellaert99} is a Monte Carlo method for
robot localization.
It is a version of Markov localization, that approximates the belief
using a set of samples drawn from it.

Advantages of MCL include ability to localize the robot globally and the related
possibility to represent arbitrary shapes of probability densities as opposed to
Gaussian distribution in the widespread Kalman filters.
Compares to the grid based methods MCL automatically focuses most of the
computing power to the highly probable regions.
This algorithm is also relatively easy to implement and the action and sensor models
in sampling and probability forms are simpler than covariance matrices in
Kalman filters.

On the other hand, Monte Carlo Localization is more CPU intensive than Kalman filters
and the particle approximation doesn't work well with too precise sensors,
because if the observation model PDF becomes too "sharp" peaks, it has a high probability
of missing all samples during the correction phase and effectively ignoring the measurement.

Informally the basic MCL algorithm can be described as follows:
Keep \(n\) samples of hypothetical robot states.
In prediction phase move each sample according to the action performed + noise corresponding
to uncertainty on the action's result (e.g. wheel slip).
Correction calculates how probable the incoming measurement is
for every sampled state and set this probability as the sample's weight.
Finally in the resampling phase throw away samples with low weights, and
instead add copies of the highly weighted samples.

\subsection{Algorithm}

MCL keeps a set of samples \(S_i = \{s^k_{i} | k = 1,\dotsc,n\} \) drawn from
\(P(X_i = x | X_{i-1}, o_i, a_{i - 1})\) to represent the belief.

\begin{description}
\item[Prediction] \hfill \\
	\emph{
	Create a new set \(S'_i = \{s'^k_{i}\} \)
	by drawing a new sample \(s'^k_{i}\)
	from the action model \(\Prob(X_i = x \mid X_{i-1} = s^k_{i-1}, a_{i - 1})\)
	for every sample in \(S_{i-1}\).
	}

	This effectively is stratified sampling from the empirical predictive density
	\begin{equation}
		\label{eq:mcl-predictive-density}
		\hat{\Prob}(X_i = x \mid X_{i - 1}, a_{i - 1}) =
		\sum_{k = 1}^n \Prob(X_i = x \mid X_{i-1} = s^k_{i-1}, a_{i - 1})
	\end{equation}
	which is used instead of the predictive density from \eqref{eq:markov-prediction}.

\item[Correction] \hfill \\
	\emph{
	Use the sensor model to calculate weight \(w^k_i = \Prob(o_i \mid X_i = s'^k_i)\)
	for each sample in \(S'_i\).
	}

	Ideally in the correction phase the algorithm would draw samples directly from
	\(\Prob(o_i \mid X_i = x) \Prob(X_i = x \mid X_{i - 1}, a_{i - 1})\),
	however there is no easy way to achieve this.
	Instead, Monte Carlo localization 
	uses SIR (sampling / importance resampling, \cite{smith92})
	to approximate sampling from
	\(\Prob(o_i \mid X_i = x) \hat{\Prob}(X_i = x \mid X_{i - 1}, a_{i - 1})\) using the set \(S'_i\).
	This is achieved by weighting the samples as described earlier and resampling
	according to these weights in the next phase.

	The weights are obtained by dividing the target density divided by the available density:
	\begin{equation}
		w^k_i = 
		\frac{
			\Prob(o_i \mid X_i = s'^k_i) \hat{\Prob}(X_i = s'^k_i \mid X_{i - 1}, a_{i - 1})
		}{
			\hat{\Prob}(X_i =  s'^k_i \mid X_{i - 1}, a_{i - 1})
		} = \Prob(o_i \mid X_i = s'^k_i)
	\end{equation}


\item[Resampling] \hfill \\
	\emph{
	Draw \(n\) new samples \(s^k_{i+1}\) from \(S_i\) by placing
	weight \(\{w^k_i\}\) on \(s'^k_{i}\).
	}

	Resampling phase is necessary to finalize the SIR procedure from the correction phase.
	Resampling of the sample set can be done in \(\mathrm{O}(n)\), using the systematic
	resampling algorithm described in \cite{arulampalam01}.
\end{description}

\subsection{Modifications}

\subsubsection{Adaptive Sample Size}
During global localization, or tracking failure MCL needs a relatively large number
of samples to localize correctly, however when the robot is successfully localized
a much less samples is necessary.

An improvement of the MCL algorithm is to vary the sample size dynamically.
This can be done using likehood sampling \cite{fox99}, that works by 
resampling not until a fixed number of samples is
generated, but until the sum of unnormalized weights exceeds a given threshold.
Another more involved approach, using KLD-sampling algorithm
(KLD stands for Kullback-Leibler distance) is described in \cite{fox03}.

%\subsubsection{Delayed Resampling}

\subsubsection{Mixture MCL}

Mixture MCL \cite{thrun99} is a modification of Monte Carlo Localization that
solves the problem with too precise observations.
The modified algorithm uses a dual algorithm to MCL -- sampling from
observation model and correction from motion model -- and combines it with
regular Monte Carlo Localization.
