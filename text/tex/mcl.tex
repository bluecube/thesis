\addtocontents{toc}{\protect\newpage}

\chapter{Monte Carlo Localization}
\label{chap:mcl}

The following chapter defines the localization problem, provides an overview
of Markov localization and Kalman filters and finally describes
the Monte Carlo localization algorithm.

\section{Localization}
Localization of a mobile robot is a task of estimating position (and possibly
also other state) of the robot based on performed actions and sensor readings.
To navigate its environment reliably, an autonomous mobile robot needs
a relatively precise estimate of its position.

Localization methods can be categorized according to many different criteria.
One of the possibilities is the categorization based on whether the environment is static
or dynamic.

Another dimension along which the localization algorithms can be grouped
is the ability of the localization algorithm to modify behavior of the robot
in order to find the current position faster or with more precision.

Next, localization can mean either position tracking, where the robot knows its initial
position and the task is only to update the estimate and handle relatively small
errors, or global localization, where the robot has to find its position from scratch
and recover from serious localization errors.

In this work we will focus only on passive localization in static environment.
Since Monte Carlo localization is a topic of this text we will be favoring
global localization, although typically GPS is used with Kalman filters
which are only capable of tracking the estimate locally.

\section{Markov Localization}

Markov localization \cite{fox98,diard03} is a recursive probabilistic
algorithm that estimates state of the robot based on its
actions and possibly noisy measured data.

\subsection{Assumptions}
\label{sec:markov-assumptions}
Markov localization assumes that the state of the robot only depends
on the state and action performed in the previous time frame:
	\begin{equation}
		\Prob(X_i = x \mid X_{1, \dotsc, i - 1}, a_{1, \dotsc, i - 1},
		o_{1, \dotsc, i - 1}) =
		\Prob(X_i = x \mid X_{i - 1}, a_{i - 1})
	\end{equation}
and that each observation only depends on the current state:
	\begin{equation}
		\label{eq:markov-observation-independence}
		\Prob(o_i \mid X_{1, \dotsc, i}, a_{1, \dotsc, i - 1},
		o_{1, \dotsc, i - 1}) =
		\Prob(o_i \mid X_i)
	\end{equation}


\subsection{Derivation}

The algorithm computes the probability density of the robot state, called belief.
\begin{equation}
	\label{eq:markov-belief-def}
	\Bel(X_i = x) = \Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1,\dotsc,i - 1})
\end{equation}
Here \(X_i\) is the random variable representing the robot's state,
\(o_i\) is the observed sensor input in time step \(i\) and \(a_i\)
is the action the robot performs in the time step \(i\), after it
measures \(o_i\).

Using Bayes rule, \Cref{eq:markov-belief-def} can be transformed to
\begin{equation}
	\label{eq:markov-derivation1}
	\Bel(X_i = x) =
	\frac{
		\Prob(o_i \mid X_i = x, o_{1, \dotsc, i}, a_{1,\dotsc,i - 1})
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
	}{
		\Prob(o_i \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})
	}
\end{equation}

Note that the denominator \(\Prob(o_i \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})\) only
serves as a normalization constant.
In further equations it will be replaced by \(\eta^{-1}\).
Other than that, we can use the independence assumptions made at the beginning
of this section and simplify \eqref{eq:markov-derivation1} to
\begin{equation}
	\label{eq:markov-derivation2}
	\Bel(X_i = x) =
		\eta \Prob(o_i \mid X_i = x)
		\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
\end{equation}
%The equation \eqref{eq:markov-derivation1} has already been simplified using
%the independence assumptions.
By integrating over all possible states in time \(i - 1\), the rightmost term in
\Cref{eq:markov-derivation1} can be expanded in a following way:
\begin{multline}
	\label{eq:markov-derivation3}
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) = \\
	\int
	\Prob(X_i = x \mid X_{i - 1}=x', o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
	\Prob(X_{i-1} = x' \mid o_{1, \dotsc, i-1}, a_{1, \dotsc, i-1})
	\,\mathrm{d}x'
\end{multline}
After substituting the definition of belief from \Cref{eq:markov-belief-def}
and another use of the independence assumptions we get
\begin{equation}
	\label{eq:markov-derivation4}
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) =
	\int
	\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})
	\Bel(X_{i-1} = x')
	\,\mathrm{d}x'
\end{equation}
Finally, substituting into \eqref{eq:markov-derivation2} gives us the recursive
equation
\begin{equation}
	\label{eq:markov-final}
	\Bel(X_i = x) =
	\eta \Prob(o_i \mid X_i = x)
		\int
		\Prob(X_i = x' \mid X_{i - 1}=x', a_{i - 1})
		\Bel(X_{i-1} = x')
		\,\mathrm{d}x'
\end{equation}

We will call the conditional density \(\Prob(o_i \mid X_i = x)\) \emph{sensor model} and
the density \(\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})\) \emph{motion model}.
The sensor model describes the probability of observing \(o_i\) at given time.
It implicitly contains a map of the environment and models interactions of sensors with
the environment.
The action model contains information about how the robot's actions relate to changes in its state.

\subsection{Algorithm}
The algorithm for Markov localization operates in two alternating phases:
prediction phase, in witch the algorithm incorporates a performed action into the belief
(and therefore predicts the state after the action) and correction phase which updates
the measurements based on the observed sensor measurements.

\subsubsection{Prediction}
The prediction phase uses the action model to obtain predictive density based on the action performed
and the previous belief, by integrating over all possible states at time \(i - 1\).
The following equation is an exact copy of \eqref{eq:markov-derivation4}, included here for completeness.
\begin{equation}
	\label{eq:markov-prediction}
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1}) =
	\int
	\Prob(X_i = x \mid X_{i - 1}=x', a_{i - 1})
	\Bel(X_{i-1} = x')
	\,\mathrm{d}x'
\end{equation}

\subsubsection{Correction}
In the correction phase, the belief is regenerated based on the predictive density
and the sensor model.
\begin{equation}
	\label{eq:markov-correction}
	\Bel(X_i = x) =
	\eta \Prob(o_i \mid X_i = x)
	\Prob(X_i = x \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})
\end{equation}

\subsection{Density Representations}
The algorithm as described above doesn't tell us how to represent the probability
densities encountered.
Several specializations of Markov localization exist, differing mainly in how the
current state estimation is represented.
Examples include Gaussian distribution in Kalman filters,
grid based algorithms \cite{fox98}, or representing the distribution
using samples in Monte Carlo localization (see \Cref{sec:mcl-algorithm}).

\subsection{Kalman Filter}
\label{sec:kalman}
Kalman filter \cite{kalman60,welch95} is a popular state estimator, often used in robotics
for localization.
It can be viewed as a closed form solution of Markov localization where both the
action model and the measurement model are linear and Gaussian \cite{diard03}.

\begin{gather}
	\label{eq:kalman-action-model}
	\Prob(X_i = x \mid X_{i-1} = x', a_{i - 1}) \sim \normalDistribution(A_{i - 1}x' + B_{i - 1}a_{i - 1}, Q_{i - 1})
	\\
	\label{eq:kalman-measurement-model}
	\Prob(o_{i} \mid X_i = x) \sim \normalDistribution(H_{i}x, R_i)
\end{gather}
In \Cref{eq:kalman-action-model,eq:kalman-measurement-model}
the matrix \(A_i\) describes the change of state if there was no control input and no noise,
matrix \(B_i\) contains the influence of action \(a_i\) and \(H_i\) relates state to measured
values.
\(Q_i\) and \(R_i\) are covariance matrices of the action model and the measurement model.

Because the initial belief, the action model and the measurement model are all Gaussian,
belief will always remain Gaussian as well.
This means that only the mean value and covariance matrix need to be stored, making
Kalman filter very efficient, requiring only several matrix operations in each step.

Non linear motion and measurement models can be approximated
using a first order Taylor expansion to form an extended Kalman
filter (EKF, \cite{welch95}).

As an illustration of operations of a Kalman filter, the update rules are included.
For more detailed description see \cite{welch95}.

\subsubsection{Prediction}
\begin{gather}
	\bar{x}^{-}_i = A_{i - 1}\bar{x}_{i-1} + B_{i - 1}a_{i - 1}
	\\
	P^{-}_i = A_{i - 1}P_{i - 1}A^{\mathrm{T}}_{i - 1} + Q_{i - 1}
\end{gather}

Here \(\bar{x}_i\) and \(P_i\) are mean and covariance describing \(\Bel(X_i)\).
\(\bar{x}^{-}_i\) and \(P^{-}_i\) are mean and covariance of the predictive density
\(\Prob(X_i \mid o_{1, \dotsc, i}, a_{1, \dotsc, i - 1})\)

\subsubsection{Correction}
\begin{gather}
	\label{eq:kalman-gain}
	K_i = P^{-}_{i}H^{\mathrm{T}}_{i}(H_{i}P^{-}_{i}H^{\mathrm{T}}_{i} + R_{i})^{-1}
	\\
	\bar{x}_i = \bar{x}^{-}_i + K_i (o_i - H_{i}\bar{x}^{-}_i)
	\\
	P_i = (I - K_{i}H_{i})P^{-}_i
\end{gather}


\section{Monte Carlo Localization}
\label{sec:mcl-algorithm}

\begin{figure}[tp]
	\centering
	\input{img/chessboard_mcl.pdf_tex}
	\caption{Illustration of MCL sample cloud.}
	\label{fig:mcl}
\end{figure}

Monte Carlo localization (MCL, \cite{dellaert99}) is a Monte Carlo method for
robot localization.
It is a version of Markov localization, that approximates the belief
using a set of samples drawn from it.

Advantages of MCL include a
possibility to represent arbitrary shapes of probability densities as opposed to
Gaussian distribution in the widespread Kalman filters
and the related ability to localize the robot globally.
Compared to the grid based methods MCL automatically focuses most of the
computing power to the highly probable regions.
This algorithm is also fairly easy to implement and the action and sensor models
in sampling and probability forms are simpler than covariance matrices in
Kalman filters.

On the other hand, Monte Carlo localization is more CPU intensive than Kalman filters
and the particle approximation doesn't work well with too precise sensors,
because if the observation model PDF becomes too \enquote{sharp} peaks, it has a high probability
of missing all samples during the correction phase and effectively ignoring the measurement.

Informally the basic MCL can be described as follows:
Keep \(n\) samples of hypothetical robot states.
In the prediction phase move each sample according to the action performed with the noise corresponding
to uncertainty on the action's result (e.g. wheel slip) added.
Correction calculates how probable the incoming measurement is
for every sampled state and set this probability as the sample's weight.
Finally in the resampling phase throw away samples with low weights, and add
copies of the highly weighted samples instead.

\Cref{fig:mcl} shows example of two identical simulated robots on a chessboard.
Red lines show paths traveled by the robots and blue dots represent MCL samples.
Both of the robots use odometry data to estimate position and the robot on the right
senses color of the tile underneath it as well.

\subsection{Algorithm}

\begin{figure}[p]
\begin{algorithm}[H]
	\SetKwFunction{append}{append}
	\SetKwFunction{emptyList}{emptyList}
	\SetKwFor{Forever}{repeat}{}{end}
	\SetKwData{weights}{weights}
	\SetKwData{samples}{samples}
	\SetKwData{sample}{s}
	\SetKwData{ii}{i}
	\SetKwData{nn}{n}
	\SetKwData{action}{action}
	\SetKwData{observation}{observation}

	\samples \assign \initialize{}\;
	\Forever{}{
		\samples \assign \predict{\samples, \getAction{}}\;
		\weights \assign \correct{\samples, \getObservation{}}\;
		\samples \assign \resample{\samples, \weights}\;
		\BlankLine
		\outputPosition{\samples}
	}

	\BlankLine
	\BlankLine
	\BlankLine
	\function{initialize}{
		\KwIn{nothing}
		\KwOut{list of samples}
		\BlankLine
		\samples \assign \emptyList\;
		\For{\ii \assign \(1\) \KwTo \nn}{
			\append{\samples, \randomSample{}}\;
		}
		\BlankLine
		\Return{\samples}\;
	}

	\BlankLine
	\BlankLine
	\BlankLine
	\function{predict}{
		\KwIn{list of samples, performed action}
		\KwOut{modified list of samples}
		\BlankLine
		\ForEach{\sample \(\in\) \samples}{
			\sample \assign \sampleFromActionModel{\sample, \action}\;
		}
		\BlankLine
		\Return{\samples}\;
	}

	\BlankLine
	\BlankLine
	\BlankLine
	\function{correct}{
		\KwIn{list of samples, observation}
		\KwOut{list of weights}
		\BlankLine
		\weights \assign \emptyList\;
		\ForEach{\sample \(\in\) \samples}{
			\append{\weights, \observationProbability{\sample, \observation}}\;
		}
		\BlankLine
		\Return{\weights}\;
	}
\end{algorithm}
\caption{Monte Carlo localization algorithm}
\label{algo:mcl}
\end{figure}

MCL keeps a set of samples \(S_i = \{s^k_{i} \mid k = 1,\dotsc,n\} \) drawn from the distribution
\(\Prob(X_i = x \mid X_{i-1}, o_i, a_{i - 1})\) to represent the belief and performs
prediction, correction and resampling phases for every time frame.

Pseudocode in \Cref{algo:mcl} contains the basic structure of Monte Carlo localization.
The algorithm reads inputs from the environment by calling functions
\getAction{} and \getObservation{}, that return the last performed action and the
last measured observation.
Output is made available to the rest of the robot's software in function \outputPosition{}.

Prediction and correction steps in the algorithm utilize the motion
model through \sampleFromActionModel{} and sensor model using \observationProbability{}.
The function \sampleFromActionModel{} returns a random predicted sample based on a previous sample and an action,
\observationProbability{} gives the probability of an observation in a given state.

The \initialize{} function is an example of how MCL can be used to localize
a robot without knowledge of its previous location by starting with samples distributed
uniformly through the environment.

In a practical implementation it is often useful to \enquote{invert} the algorithm and
have external code pass events with performed action and sensor readings.

\subsubsection{Prediction}
\begin{itemize}
\item
Create a new sample set \(S'_i = \{s'^k_{i}\} \)
by drawing a new sample \(s'^k_{i}\)
from the action model \(\Prob(X_i = x \mid X_{i-1} = s^k_{i-1}, a_{i - 1})\)
for every sample in \(S_{i-1}\).
\end{itemize}

This is stratified sampling from the empirical predictive density
\begin{equation}
	\label{eq:mcl-predictive-density}
	\hat{\Prob}(X_i = x \mid X_{i - 1}, a_{i - 1}) =
	\sum_{k = 1}^n \Prob(X_i = x \mid X_{i-1} = s^k_{i-1}, a_{i - 1})
\end{equation}
which is used instead of the predictive density from \eqref{eq:markov-prediction}.

\subsubsection{Correction}
\begin{itemize}
\item
Using the sensor model, calculate weight \(w^k_i = \Prob(o_i \mid X_i = s'^k_i)\)
for each sample \(s'^k_i\) in the predictive sample set \(S'_i\).
\end{itemize}

Ideally in the correction phase the algorithm would draw samples directly from
\(\Prob(o_i \mid X_i = x) \Prob(X_i = x \mid X_{i - 1}, a_{i - 1})\),
however there is no easy way to achieve this.
Instead, Monte Carlo localization 
uses sampling / importance resampling (SIR, \cite{smith92})
to approximate sampling from
\(\Prob(o_i \mid X_i = x) \hat{\Prob}(X_i = x \mid X_{i - 1}, a_{i - 1})\) using the set \(S'_i\).
This is achieved by weighting the samples as described earlier and resampling
according to these weights in the next phase.

The weights are obtained by dividing the target density by the available density:
\begin{equation}
	w^k_i = 
	\frac{
		\Prob(o_i \mid X_i = s'^k_i) \hat{\Prob}(X_i = s'^k_i \mid X_{i - 1}, a_{i - 1})
	}{
		\hat{\Prob}(X_i =  s'^k_i \mid X_{i - 1}, a_{i - 1})
	} = \Prob(o_i \mid X_i = s'^k_i)
\end{equation}

\subsubsection{Resampling}
\begin{itemize}
\item
Draw \(n\) new samples \(s^k_{i+1}\) from \(S'_i\) by placing
weight \(\{w^k_i\}\) on \(s'^k_{i}\).
\end{itemize}

The resampling phase is necessary to finalize the SIR procedure from the correction phase.
It can be performed in \(\mathrm{O}(n)\), using the systematic
resampling algorithm described in \cite{arulampalam01}.

\subsection{Modifications}

\subsubsection{Adaptive Sample Size}
During global localization or tracking failure MCL needs a relatively large number
of samples to localize correctly.
The required number of samples, however, decreases during position keeping.

An improvement of the MCL algorithm is to vary the sample size dynamically.
This can be done by using likehood sampling \cite{fox99}, that works by
resampling not until a fixed number of samples is
generated, but until the sum of unnormalized weights exceeds a given threshold.
Another, more involved approach, using Kullback-Leibler distance in KLD-sampling algorithm
is described in \cite{fox03}.

\subsubsection{Delayed Resampling}
It is possible to keep sample weights during the whole operation of the algorithm.
This means that the resampling phase can be performed only once for several prediction and correction steps
for example to save computing power.
Various criteria for starting resampling may be chosen, examples include a fixed number of
prediction / correction steps, or when the effective number of particles drops under a predefined threshold.
This is explained in detail in \cite{arulampalam01}.

\subsubsection{Mixture MCL}

Mixture MCL \cite{thrun99} is a modification of Monte Carlo localization that
solves the problem with too precise observations.
The modified algorithm uses a dual algorithm to MCL -- sampling from
the observation model and correction from the motion model -- and combines it with
regular Monte Carlo localization.
